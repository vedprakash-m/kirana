name: Performance Regression Gates

on:
  pull_request:
    branches: [main, develop]
    paths:
      - 'backend/**'
      - 'frontend/src/**'
  push:
    branches: [main]

jobs:
  performance-tests:
    name: Run Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      # Local Cosmos DB Emulator (or use Azure Cosmos DB test instance)
      cosmos:
        image: mcr.microsoft.com/cosmosdb/linux/azure-cosmos-emulator:latest
        ports:
          - 8081:8081
        env:
          AZURE_COSMOS_EMULATOR_PARTITION_COUNT: 2
          AZURE_COSMOS_EMULATOR_ENABLE_DATA_PERSISTENCE: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for baseline comparison

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Backend Dependencies
        run: cd backend && npm ci

      - name: Install Frontend Dependencies
        run: cd frontend && npm ci

      - name: Install Performance Testing Tools
        run: |
          npm install -g autocannon artillery clinic

      - name: Start Backend (Azure Functions)
        run: |
          cd backend
          npm start &
          
          # Wait for Functions to start (max 30s)
          timeout 30 bash -c 'until curl -s http://localhost:7071/api/health > /dev/null; do sleep 1; done' || echo "Functions started"
        env:
          COSMOS_DB_CONNECTION_STRING: AccountEndpoint=https://localhost:8081/;AccountKey=C2y6yDjf5/R+ob0N8A7Cgv30VRDJIWEHLM+4QDU5DE2nQ9nDuVTqobD4b8mGGyPMbIZnqyMsEcaGQy67XIw/Jw==
          GOOGLE_AI_API_KEY: ${{ secrets.GOOGLE_AI_API_KEY_TEST }}
          ENABLE_LLM: false  # Disable LLM for performance tests

      - name: Seed Test Data
        run: |
          cd backend
          node scripts/seed-perf-test-data.js
        env:
          COSMOS_DB_CONNECTION_STRING: AccountEndpoint=https://localhost:8081/;AccountKey=C2y6yDjf5/R+ob0N8A7Cgv30VRDJIWEHLM+4QDU5DE2nQ9nDuVTqobD4b8mGGyPMbIZnqyMsEcaGQy67XIw/Jw==

      - name: Run Performance Tests (Backend)
        id: backend-perf
        run: |
          cd backend
          
          # Test 1: GET /api/items (most common endpoint)
          echo "Testing GET /items..."
          autocannon -c 10 -d 30 -j http://localhost:7071/api/items?householdId=test-user > perf-get-items.json
          
          # Test 2: POST /api/items (write performance)
          echo "Testing POST /items..."
          autocannon -c 5 -d 20 -m POST -H "Content-Type: application/json" -b '{"itemName":"Test Item","category":"PRODUCE"}' -j http://localhost:7071/api/items > perf-post-items.json
          
          # Test 3: Prediction calculation (compute-intensive)
          echo "Testing POST /predictions/calculate..."
          autocannon -c 5 -d 20 -m POST -H "Content-Type: application/json" -b '{"itemIds":["item-1","item-2","item-3"]}' -j http://localhost:7071/api/predictions/calculate > perf-predictions.json
          
          # Extract metrics
          GET_P95=$(jq '.latency.p95' perf-get-items.json)
          GET_P99=$(jq '.latency.p99' perf-get-items.json)
          GET_AVG=$(jq '.latency.mean' perf-get-items.json)
          GET_RPS=$(jq '.requests.average' perf-get-items.json)
          
          POST_P95=$(jq '.latency.p95' perf-post-items.json)
          POST_P99=$(jq '.latency.p99' perf-post-items.json)
          
          PRED_P95=$(jq '.latency.p95' perf-predictions.json)
          PRED_P99=$(jq '.latency.p99' perf-predictions.json)
          
          # Output for next step
          echo "get_p95=$GET_P95" >> $GITHUB_OUTPUT
          echo "get_p99=$GET_P99" >> $GITHUB_OUTPUT
          echo "get_avg=$GET_AVG" >> $GITHUB_OUTPUT
          echo "get_rps=$GET_RPS" >> $GITHUB_OUTPUT
          echo "post_p95=$POST_P95" >> $GITHUB_OUTPUT
          echo "post_p99=$POST_P99" >> $GITHUB_OUTPUT
          echo "pred_p95=$PRED_P95" >> $GITHUB_OUTPUT
          echo "pred_p99=$PRED_P99" >> $GITHUB_OUTPUT

      - name: Run Performance Tests (Frontend)
        id: frontend-perf
        run: |
          cd frontend
          npm run build
          
          # Start static server
          npx serve -s dist -p 3000 &
          sleep 5
          
          # Lighthouse CI for frontend performance
          npm install -g @lhci/cli
          
          lhci autorun --config=.lighthouserc.json || true
          
          # Extract performance score
          PERF_SCORE=$(jq '.[] | select(.categories.performance) | .categories.performance.score' .lighthouseci/lhr-*.json | head -1)
          FCP=$(jq '.[] | select(.audits["first-contentful-paint"]) | .audits["first-contentful-paint"].numericValue' .lighthouseci/lhr-*.json | head -1)
          TTI=$(jq '.[] | select(.audits.interactive) | .audits.interactive.numericValue' .lighthouseci/lhr-*.json | head -1)
          
          echo "perf_score=$PERF_SCORE" >> $GITHUB_OUTPUT
          echo "fcp=$FCP" >> $GITHUB_OUTPUT
          echo "tti=$TTI" >> $GITHUB_OUTPUT

      - name: Load Performance Baseline
        id: baseline
        run: |
          # Check if baseline exists
          if [ -f performance-baseline.json ]; then
            BASELINE=$(cat performance-baseline.json)
          else
            # Create default baseline (first run)
            BASELINE='{
              "backend": {
                "get_items_p95": 450,
                "get_items_p99": 800,
                "get_items_avg": 200,
                "get_items_rps": 100,
                "post_items_p95": 500,
                "post_items_p99": 900,
                "predictions_p95": 1200,
                "predictions_p99": 2000
              },
              "frontend": {
                "perf_score": 0.90,
                "fcp": 1500,
                "tti": 3000
              }
            }'
          fi
          
          echo "baseline<<EOF" >> $GITHUB_OUTPUT
          echo "$BASELINE" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Compare Against Baseline
        id: compare
        run: |
          BASELINE='${{ steps.baseline.outputs.baseline }}'
          
          # Backend metrics
          GET_P95=${{ steps.backend-perf.outputs.get_p95 }}
          GET_P99=${{ steps.backend-perf.outputs.get_p99 }}
          POST_P95=${{ steps.backend-perf.outputs.post_p95 }}
          PRED_P95=${{ steps.backend-perf.outputs.pred_p95 }}
          
          # Frontend metrics
          PERF_SCORE=${{ steps.frontend-perf.outputs.perf_score }}
          FCP=${{ steps.frontend-perf.outputs.fcp }}
          TTI=${{ steps.frontend-perf.outputs.tti }}
          
          # Extract baseline values
          BASELINE_GET_P95=$(echo $BASELINE | jq -r '.backend.get_items_p95')
          BASELINE_GET_P99=$(echo $BASELINE | jq -r '.backend.get_items_p99')
          BASELINE_POST_P95=$(echo $BASELINE | jq -r '.backend.post_items_p95')
          BASELINE_PRED_P95=$(echo $BASELINE | jq -r '.backend.predictions_p95')
          BASELINE_PERF_SCORE=$(echo $BASELINE | jq -r '.frontend.perf_score')
          BASELINE_FCP=$(echo $BASELINE | jq -r '.frontend.fcp')
          BASELINE_TTI=$(echo $BASELINE | jq -r '.frontend.tti')
          
          # Calculate regression percentages (using bc for floating point)
          GET_P95_REGRESSION=$(echo "scale=2; (($GET_P95 - $BASELINE_GET_P95) / $BASELINE_GET_P95) * 100" | bc)
          GET_P99_REGRESSION=$(echo "scale=2; (($GET_P99 - $BASELINE_GET_P99) / $BASELINE_GET_P99) * 100" | bc)
          POST_P95_REGRESSION=$(echo "scale=2; (($POST_P95 - $BASELINE_POST_P95) / $BASELINE_POST_P95) * 100" | bc)
          PRED_P95_REGRESSION=$(echo "scale=2; (($PRED_P95 - $BASELINE_PRED_P95) / $BASELINE_PRED_P95) * 100" | bc)
          FCP_REGRESSION=$(echo "scale=2; (($FCP - $BASELINE_FCP) / $BASELINE_FCP) * 100" | bc)
          TTI_REGRESSION=$(echo "scale=2; (($TTI - $BASELINE_TTI) / $BASELINE_TTI) * 100" | bc)
          PERF_SCORE_REGRESSION=$(echo "scale=2; (($BASELINE_PERF_SCORE - $PERF_SCORE) / $BASELINE_PERF_SCORE) * 100" | bc)
          
          # Output for next step
          echo "get_p95_regression=$GET_P95_REGRESSION" >> $GITHUB_OUTPUT
          echo "get_p99_regression=$GET_P99_REGRESSION" >> $GITHUB_OUTPUT
          echo "post_p95_regression=$POST_P95_REGRESSION" >> $GITHUB_OUTPUT
          echo "pred_p95_regression=$PRED_P95_REGRESSION" >> $GITHUB_OUTPUT
          echo "fcp_regression=$FCP_REGRESSION" >> $GITHUB_OUTPUT
          echo "tti_regression=$TTI_REGRESSION" >> $GITHUB_OUTPUT
          echo "perf_score_regression=$PERF_SCORE_REGRESSION" >> $GITHUB_OUTPUT
          
          # Determine if regression is acceptable (<20% threshold)
          THRESHOLD=20
          FAILED=false
          
          if (( $(echo "$GET_P95_REGRESSION > $THRESHOLD" | bc -l) )); then
            echo "‚ùå GET /items p95 regression: +$GET_P95_REGRESSION% (threshold: $THRESHOLD%)"
            FAILED=true
          fi
          
          if (( $(echo "$GET_P99_REGRESSION > $THRESHOLD" | bc -l) )); then
            echo "‚ùå GET /items p99 regression: +$GET_P99_REGRESSION% (threshold: $THRESHOLD%)"
            FAILED=true
          fi
          
          if (( $(echo "$POST_P95_REGRESSION > $THRESHOLD" | bc -l) )); then
            echo "‚ùå POST /items p95 regression: +$POST_P95_REGRESSION% (threshold: $THRESHOLD%)"
            FAILED=true
          fi
          
          if (( $(echo "$PRED_P95_REGRESSION > $THRESHOLD" | bc -l) )); then
            echo "‚ùå Predictions p95 regression: +$PRED_P95_REGRESSION% (threshold: $THRESHOLD%)"
            FAILED=true
          fi
          
          if (( $(echo "$FCP_REGRESSION > $THRESHOLD" | bc -l) )); then
            echo "‚ùå First Contentful Paint regression: +$FCP_REGRESSION% (threshold: $THRESHOLD%)"
            FAILED=true
          fi
          
          if (( $(echo "$TTI_REGRESSION > $THRESHOLD" | bc -l) )); then
            echo "‚ùå Time to Interactive regression: +$TTI_REGRESSION% (threshold: $THRESHOLD%)"
            FAILED=true
          fi
          
          if (( $(echo "$PERF_SCORE_REGRESSION > 10" | bc -l) )); then
            echo "‚ùå Lighthouse performance score regression: -$PERF_SCORE_REGRESSION% (threshold: 10%)"
            FAILED=true
          fi
          
          if [ "$FAILED" = true ]; then
            echo "regression_detected=true" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "‚úÖ All performance metrics within acceptable range"
            echo "regression_detected=false" >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const getP95 = parseFloat('${{ steps.backend-perf.outputs.get_p95 }}');
            const getP99 = parseFloat('${{ steps.backend-perf.outputs.get_p99 }}');
            const getAvg = parseFloat('${{ steps.backend-perf.outputs.get_avg }}');
            const getRps = parseFloat('${{ steps.backend-perf.outputs.get_rps }}');
            const postP95 = parseFloat('${{ steps.backend-perf.outputs.post_p95 }}');
            const predP95 = parseFloat('${{ steps.backend-perf.outputs.pred_p95 }}');
            const perfScore = parseFloat('${{ steps.frontend-perf.outputs.perf_score }}') * 100;
            const fcp = parseFloat('${{ steps.frontend-perf.outputs.fcp }}');
            const tti = parseFloat('${{ steps.frontend-perf.outputs.tti }}');
            
            const getP95Regression = parseFloat('${{ steps.compare.outputs.get_p95_regression }}');
            const getP99Regression = parseFloat('${{ steps.compare.outputs.get_p99_regression }}');
            const postP95Regression = parseFloat('${{ steps.compare.outputs.post_p95_regression }}');
            const predP95Regression = parseFloat('${{ steps.compare.outputs.pred_p95_regression }}');
            const fcpRegression = parseFloat('${{ steps.compare.outputs.fcp_regression }}');
            const ttiRegression = parseFloat('${{ steps.compare.outputs.tti_regression }}');
            const perfScoreRegression = parseFloat('${{ steps.compare.outputs.perf_score_regression }}');
            
            const regressionDetected = '${{ steps.compare.outputs.regression_detected }}' === 'true';
            
            const statusEmoji = regressionDetected ? '‚ùå' : '‚úÖ';
            const statusText = regressionDetected ? 'Performance Regression Detected' : 'Performance Within Limits';
            
            const formatRegression = (value) => {
              const sign = value >= 0 ? '+' : '';
              const emoji = value > 20 ? 'üî¥' : value > 10 ? 'üü°' : 'üü¢';
              return `${emoji} ${sign}${value.toFixed(1)}%`;
            };
            
            const comment = `## ${statusEmoji} Performance Test Results
            
            ### Backend API Performance
            
            | Endpoint | Metric | Current | Baseline | Change |
            |----------|--------|---------|----------|--------|
            | GET /items | p95 latency | ${getP95.toFixed(0)}ms | ${(getP95 / (1 + getP95Regression/100)).toFixed(0)}ms | ${formatRegression(getP95Regression)} |
            | GET /items | p99 latency | ${getP99.toFixed(0)}ms | ${(getP99 / (1 + getP99Regression/100)).toFixed(0)}ms | ${formatRegression(getP99Regression)} |
            | GET /items | avg latency | ${getAvg.toFixed(0)}ms | - | - |
            | GET /items | throughput | ${getRps.toFixed(0)} req/s | - | - |
            | POST /items | p95 latency | ${postP95.toFixed(0)}ms | ${(postP95 / (1 + postP95Regression/100)).toFixed(0)}ms | ${formatRegression(postP95Regression)} |
            | Predictions | p95 latency | ${predP95.toFixed(0)}ms | ${(predP95 / (1 + predP95Regression/100)).toFixed(0)}ms | ${formatRegression(predP95Regression)} |
            
            ### Frontend Performance (Lighthouse)
            
            | Metric | Current | Baseline | Change |
            |--------|---------|----------|--------|
            | Performance Score | ${perfScore.toFixed(0)}/100 | ${(perfScore * (1 + perfScoreRegression/100)).toFixed(0)}/100 | ${formatRegression(-perfScoreRegression)} |
            | First Contentful Paint | ${fcp.toFixed(0)}ms | ${(fcp / (1 + fcpRegression/100)).toFixed(0)}ms | ${formatRegression(fcpRegression)} |
            | Time to Interactive | ${tti.toFixed(0)}ms | ${(tti / (1 + ttiRegression/100)).toFixed(0)}ms | ${formatRegression(ttiRegression)} |
            
            ### Summary
            
            ${regressionDetected ? 
              '‚ö†Ô∏è **Performance regression detected!** One or more metrics exceeded the 20% threshold. Please review and optimize before merging.' :
              '‚úÖ **All performance metrics within acceptable range.** No significant regression detected.'
            }
            
            **Threshold:** ¬±20% regression allowed  
            **Test Duration:** 30 seconds per endpoint  
            **Concurrency:** 10 connections (GET), 5 connections (POST/Predictions)
            
            <details>
            <summary>View Full Performance Report</summary>
            
            \`\`\`json
            {
              "backend": {
                "get_items": {
                  "p95": ${getP95.toFixed(2)},
                  "p99": ${getP99.toFixed(2)},
                  "avg": ${getAvg.toFixed(2)},
                  "rps": ${getRps.toFixed(2)}
                },
                "post_items": {
                  "p95": ${postP95.toFixed(2)}
                },
                "predictions": {
                  "p95": ${predP95.toFixed(2)}
                }
              },
              "frontend": {
                "performance_score": ${perfScore.toFixed(2)},
                "fcp": ${fcp.toFixed(2)},
                "tti": ${tti.toFixed(2)}
              }
            }
            \`\`\`
            
            </details>
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Update Performance Baseline (on main push)
        if: github.ref == 'refs/heads/main' && steps.compare.outputs.regression_detected == 'false'
        run: |
          # Update baseline with new measurements
          cat > performance-baseline.json << EOF
          {
            "backend": {
              "get_items_p95": ${{ steps.backend-perf.outputs.get_p95 }},
              "get_items_p99": ${{ steps.backend-perf.outputs.get_p99 }},
              "get_items_avg": ${{ steps.backend-perf.outputs.get_avg }},
              "get_items_rps": ${{ steps.backend-perf.outputs.get_rps }},
              "post_items_p95": ${{ steps.backend-perf.outputs.post_p95 }},
              "post_items_p99": ${{ steps.backend-perf.outputs.post_p99 }},
              "predictions_p95": ${{ steps.backend-perf.outputs.pred_p95 }},
              "predictions_p99": ${{ steps.backend-perf.outputs.pred_p99 }}
            },
            "frontend": {
              "perf_score": ${{ steps.frontend-perf.outputs.perf_score }},
              "fcp": ${{ steps.frontend-perf.outputs.fcp }},
              "tti": ${{ steps.frontend-perf.outputs.tti }}
            },
            "updated_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF
          
          # Commit baseline update
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add performance-baseline.json
          git commit -m "chore: update performance baseline [skip ci]"
          git push

      - name: Workflow Summary
        run: |
          echo "## ‚ö° Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.compare.outputs.regression_detected == 'false' && '‚úÖ Pass' || '‚ùå Regression Detected' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Backend Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- GET /items p95: ${{ steps.backend-perf.outputs.get_p95 }}ms (${{ steps.compare.outputs.get_p95_regression }}%)" >> $GITHUB_STEP_SUMMARY
          echo "- POST /items p95: ${{ steps.backend-perf.outputs.post_p95 }}ms (${{ steps.compare.outputs.post_p95_regression }}%)" >> $GITHUB_STEP_SUMMARY
          echo "- Predictions p95: ${{ steps.backend-perf.outputs.pred_p95 }}ms (${{ steps.compare.outputs.pred_p95_regression }}%)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Frontend Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- Performance Score: ${{ steps.frontend-perf.outputs.perf_score }}" >> $GITHUB_STEP_SUMMARY
          echo "- FCP: ${{ steps.frontend-perf.outputs.fcp }}ms (${{ steps.compare.outputs.fcp_regression }}%)" >> $GITHUB_STEP_SUMMARY
          echo "- TTI: ${{ steps.frontend-perf.outputs.tti }}ms (${{ steps.compare.outputs.tti_regression }}%)" >> $GITHUB_STEP_SUMMARY
